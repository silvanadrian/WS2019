{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Data Analytics Summer School 3 -- NLP Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preliminaries and Task Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve a task called \"stance detection\", which is about classifying the attitude of a sentence towards a concept. Read more about the task here: http://alt.qcri.org/semeval2016/task6/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readwrite.reader import *\n",
    "from readwrite.writer import *\n",
    "\n",
    "fp = \"data/semeval/\"\n",
    "train_path = fp + \"semeval2016-task6-train+dev.txt\"\n",
    "test_path = fp + \"SemEval2016-Task6-subtaskB-testdata-gold.txt\"\n",
    "pred_path = fp + \"SemEval2016-Task6-subtaskB-testdata-pred.txt\"\n",
    "tweets_train, targets_train, labels_train, ids_train = readTweetsOfficial(train_path)\n",
    "tweets_test, targets_test, labels_test, ids_test = readTweetsOfficial(test_path)\n",
    "print(tweets_train[0], targets_train[0], labels_train[0])\n",
    "print(tweets_train[721], targets_train[721], labels_train[721])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each instance consists of a tweet, a target, for which we want to predict a label (\"`FAVOR, AGAINST, NONE`\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second approach uses a pre-implemented classifier and feature extractor from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first merge the tweets and targets for easier feature extraction\n",
    "tweets_targets_train = [\" | \".join([tweets_train[i], targets_train[i]]) for i in range(len(tweets_train))]\n",
    "tweets_targets_test = [\" | \".join([tweets_test[i], targets_test[i]]) for i in range(len(tweets_test))]\n",
    "tweets_targets_train[0], labels_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform the instances into features using sklearn's count vectoriser that assigns an ID to each word, then weighs them based on their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv.fit(tweets_train)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = cv.transform(tweets_targets_train)\n",
    "features_test = cv.transform(tweets_targets_test)\n",
    "print(features_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define and train a simple logistic regression model with L2 regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s(f(x), g(x)) + loss function handled by this model\n",
    "model = LogisticRegression(penalty='l2')\n",
    "model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(features_test)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: inspect the predictions and check for which examples incorrect vs. correct features are made. Inspect which features are good vs. bad predictors of the test set instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we did overall and compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(labels_test, predictions))\n",
    "print(set(labels_test))\n",
    "print(set(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at which labels were often confused with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: try to understand the confusion matrix and think about what would cause the results you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3: Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our third approach is to use word embeddings, which are trained using a simple feed-forward neural network. Word embeddings are commonly used in NLP, so there are many ready-made software packages, the most common one of which is word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While scikit-learn did all the preprocessing and feature extraction for us, we now have to put in a little bit more work for this.\n",
    "First, we tokenise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ex3_word2vec.tokenize_tweets import tokenise_tweets\n",
    "#tweet_tokens = tokenise_tweets(tweets_train)\n",
    "#target_tokens = tokenise_tweets(targets_train)\n",
    "#tweet_tokens_test = tokenise_tweets(tweets_test)\n",
    "#target_tokens_test = tokenise_tweets(targets_test)\n",
    "tweets_targets_train_tokens = tokenise_tweets(tweets_targets_train)\n",
    "tweets_targets_test_tokens = tokenise_tweets(tweets_targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to convert labels to indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2Indeces(labels):\n",
    "    labels_ret = []\n",
    "    for i, lab in enumerate(labels):\n",
    "        if lab == 'NONE':\n",
    "            labels_ret.append(0)\n",
    "        elif lab == 'FAVOR':\n",
    "            labels_ret.append(2)\n",
    "        elif lab == 'AGAINST':\n",
    "            labels_ret.append(1)\n",
    "    return labels_ret\n",
    "\n",
    "labels_train_idx = label2Indeces(labels_train)\n",
    "labels_test_idx = label2Indeces(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to train a word2vec model. We first turn on logging to monitor the training process and set the word2vec model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# set params\n",
    "num_features = 100    # Word vector dimensionality\n",
    "min_word_count = 2   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "trainalgo = 1 # cbow: 0 / skip-gram: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the word2vec `gensim` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the package is not found, uncomment and run the line below to install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(tweets_targets_train_tokens, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg = trainalgo)\n",
    "\n",
    "# add for memory efficiency\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save the model\n",
    "model.save(\"models/skip_nostop_sing_100features_5minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can example what the word2vec model has learned.\n",
    "\n",
    "Exercise: play around with the three functions below by inputting diifferent words. What do you observe? \n",
    "Hint: you can access the model's vocabulary with \"`model.wv.vocab`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, load a word2vec model\n",
    "# model = word2vec.Word2Vec.load(modelname)\n",
    "\n",
    "# find most similar n words to given word\n",
    "def applyWord2VecMostSimilar(model, word=\"#abortion\", top=20):\n",
    "    print(\"Find \", top, \" terms most similar to \", word, \"...\")\n",
    "    for res in model.wv.most_similar(word, topn=top):\n",
    "        print(res)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# determine similarity between words\n",
    "def applyWord2VecSimilarityBetweenWords(model, w1=\"trump\", w2=\"conservative\"):\n",
    "    print(\"Computing similarity between \", w1, \" and \", w2, \"...\")\n",
    "    print(model.wv.similarity(w1, w2), \"\\n\")\n",
    "    \n",
    "# search which words/phrases the model knows which contain a searchterm\n",
    "def applyWord2VecFindWord(model, searchterm=\"trump\"):\n",
    "    print(\"Finding terms containing \", searchterm, \"...\")\n",
    "    for v in model.wv.vocab:\n",
    "        if searchterm in v:\n",
    "            print(v)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "applyWord2VecMostSimilar(model)\n",
    "applyWord2VecSimilarityBetweenWords(model)\n",
    "applyWord2VecFindWord(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: there's another gensim package that automatically detects phrases, which can be a useful preprocessing step. Train such a model and see what it learns. Here is how to train one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram = Phrases(tweets_targets_train_tokens)\n",
    "# bigram.save(\"models/phrases.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** (try at home): An alternative is to use word embeddings pre-trained on a larger dataset. Here's how to import word2vec embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained word embeddings: $ wget https://www.dropbox.com/s/bnm0trligffakd9/GoogleNews-vectors-negative300.bin.gz\n",
    "# load them\n",
    "# w2vmodel = word2vec.Word2Vec.load_word2vec_format(w2vpath, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the word embeddings as features for a stance detection model.\n",
    "Because word embeddings encode words, but each of our instances consists of more than one word, we need to apply some additional function to convert this list of word vectors into something we can use as input to our stance detection model. A simple approach is to bag of word embeddings, which is to merely average all word embeddings for a sentence / instance. This can be implemented in a few lines of code using the Python numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeSentW2V(w2vmodel, sents, dim=100):\n",
    "\n",
    "    feats = []\n",
    "    # for each tweet, get the word vectors and average them\n",
    "    for i, tweet in enumerate(sents):\n",
    "        numvects = 0\n",
    "        vect = []\n",
    "        for token in tweet:\n",
    "            try:\n",
    "                s = w2vmodel.wv[token]\n",
    "                vect.append(s)\n",
    "                numvects += 1\n",
    "            except KeyError:\n",
    "                s = 0.0\n",
    "        if vect.__len__() > 0:\n",
    "            mtrmean = np.average(vect, axis=0)\n",
    "            if i == 0:\n",
    "                feats = mtrmean\n",
    "            else:\n",
    "                feats = np.vstack((feats, mtrmean))\n",
    "        else:\n",
    "            feats = np.vstack((feats, np.zeros(dim)))\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises** (optional): \n",
    "- understand what each line in the above code does\n",
    "- write an alternative function to the above that encodes tweets and targets separately and concatenates their representations\n",
    "- write an alternative function to the above that encodes tweets and targets separately and concatenates their representations, then also concatenates the outer product between the vectors to the tweet-target representation to capture the interaction between tweets and targets\n",
    "\n",
    "Now we'll convert each training and testing instance to features, using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_w2v = encodeSentW2V(model, tweets_targets_train_tokens)\n",
    "features_test_w2v = encodeSentW2V(model, tweets_targets_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a logistic regression classifier with l2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2')\n",
    "model.fit(features_train_w2v, labels_train_idx)\n",
    "preds = model.predict(features_test_w2v)\n",
    "preds_prob = model.predict_proba(features_test_w2v)\n",
    "coef = model.coef_\n",
    "print(\"Label options\", model.classes_)\n",
    "print(\"Labels\", labels_train_idx)\n",
    "print(\"Predictions\", preds)\n",
    "print(\"Predictions probabilities\", preds_prob)\n",
    "print(\"Feat length \", features_train_w2v[0].__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the performance again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test_idx, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "- as for the model in Part 2, examine the correct and incorrect predictions. How do the results compare to the ones you obtained in Part 2?\n",
    "- replace the logistic regression classifier with a simple neural network, a multi-layer perceptron (Hint: see http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) and compare performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have trained models that ingore word order. We will now train RNNs, that take as input the word embeddings we have trained in Part 3 and learn to construct a sentence, then predict a stance label.\n",
    "\n",
    "Some more intricate pre-processing than in the previous part is necessary to map words to IDs and account for unseen words at test time. For now, let's assume we have a function that takes care of this.\n",
    "\n",
    "First, let's define some preliminaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readwrite.reader import *\n",
    "from readwrite.writer import *\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from ex4_rnns.tensoriser import prepare_data\n",
    "from ex4_rnns.batch import get_feed_dicts\n",
    "from ex4_rnns.map import numpify\n",
    "\n",
    "# Set initial random seed so results are more stable\n",
    "np.random.seed(1337)\n",
    "tf.set_random_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define various options for training our models, which have a big impact on performance. For now, let's set them to values that allow us to do rapid prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model options / hyperparameters\n",
    "options = {\"main_num_layers\": 1, \"model_type\": \"tweet-only-lstm\", \"batch_size\": 32, \"emb_dim\": 16, \n",
    "            \"max_epochs\": 50, \"skip_connections\": False, \"learning_rate\": 0.001, \"dropout_rate\": 1.0, \n",
    "            \"rnn_cell_type\": \"lstm\", \"attention\": False, \"pretr_word_embs\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define placeholders, which define what shape the data we pass on to the optmiser has.\n",
    "In our case, our data consists of instance IDs, tweets, targets and labels. For tweets and targets, we also need to provide how long the instance are, i.e. how many tokens each sentence is made up of. This is important for the RNN later on -- because an unrolled RNN consists of several time steps, one step for each token, we need to know exactly how many time steps we need for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_placeholders():\n",
    "    ids = tf.placeholder(tf.int32, [None], name=\"ids\")\n",
    "    tweets = tf.placeholder(tf.int32, [None, None], name=\"tweets\")\n",
    "    tweet_lengths = tf.placeholder(tf.int32, [None], name=\"tweets_lengths\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "    target_lengths = tf.placeholder(tf.int32, [None], name=\"targets_lengths\")\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name=\"labels\")\n",
    "    placeholders = {\"ids\": ids, \"tweets\": tweets, \"tweets_lengths\": tweet_lengths, \"targets\": targets, \"targets_lengths\": target_lengths, \"labels\": labels}\n",
    "    return placeholders\n",
    "\n",
    "placeholders = set_placeholders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data, turn it into indeces and then tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ex4_rnns.classifier_rnns import loadData\n",
    "data_train, data_test, vocab, labels = loadData(train_path, test_path, placeholders, **options)\n",
    "print(\"Data loaded and tensorised.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start defining our first model, a bidirectional RNN. In a quite most basic form with LSTM cells, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_simple(inputs, lengths, output_size, scope=None):\n",
    "    \"\"\"Dynamic bi-LSTM reader.\n",
    "\n",
    "    Args:\n",
    "        inputs (tensor): The inputs into the bi-LSTM\n",
    "        lengths (tensor): The lengths of the sequences\n",
    "        output_size (int): Size of the LSTM state of the reader\n",
    "        scope (string): The TensorFlow scope for the reader.\n",
    "\n",
    "    Returns:\n",
    "        Outputs (tensor): The outputs from the bi-LSTM.\n",
    "        States (tensor): The cell states from the bi-LSTM.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope or \"reader\", reuse=tf.AUTO_REUSE) as varscope:\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            inputs,\n",
    "            sequence_length=lengths,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "    # ( (outputs_fw,outputs_bw) , (output_state_fw,output_state_bw) )\n",
    "    # in case LSTMCell: output_state_fw = (c_fw,h_fw), and output_state_bw = (c_bw,h_bw)\n",
    "    # each [batch_size x max_seq_length x output_size]\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to define what cells we want to have for the forwards backwards and backwards reading, and then define a `tf.nn.bidirectional_dynamic_rnn`. The latter takes as arguments the forwards and backwards cells, the inputs to the RNN, i.e. a sentence, and the sequence lengths, i.e. the token length of the sentence.\n",
    "\n",
    "Let's define another function for reading a sentence with an RNN now, but with a few additional bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(inputs, lengths, output_size, contexts=(None, None), scope=None, **options):\n",
    "    \"\"\"Dynamic bi-LSTM reader; can be conditioned with initial state of other rnn.\n",
    "\n",
    "    Args:\n",
    "        inputs (tensor): The inputs into the bi-LSTM\n",
    "        lengths (tensor): The lengths of the sequences\n",
    "        output_size (int): Size of the LSTM state of the reader.\n",
    "        context (tensor=None, tensor=None): Tuple of initial (forward, backward) states\n",
    "                                  for the LSTM\n",
    "        scope (string): The TensorFlow scope for the reader.\n",
    "\n",
    "    Returns:\n",
    "        Outputs (tensor): The outputs from the bi-LSTM.\n",
    "        States (tensor): The cell states from the bi-LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    skip_connections = options[\"skip_connections\"]\n",
    "    attention = options[\"attention\"]\n",
    "    num_layers = options[\"main_num_layers\"]\n",
    "    drop_keep_prob = options[\"dropout_rate\"]\n",
    "\n",
    "    with tf.variable_scope(scope or \"reader\", reuse=tf.AUTO_REUSE) as varscope:\n",
    "        if options[\"rnn_cell_type\"] == \"layer_norm\":\n",
    "            cell_fw = tf.contrib.rnn.LayerNormBasicLSTMCell(output_size)\n",
    "            cell_bw = tf.contrib.rnn.LayerNormBasicLSTMCell(output_size)\n",
    "        elif options[\"rnn_cell_type\"] == \"nas\":\n",
    "            cell_fw = tf.contrib.rnn.NASCell(output_size)\n",
    "            cell_bw = tf.contrib.rnn.NASCell(output_size)\n",
    "        elif options[\"rnn_cell_type\"] == \"phasedlstm\":\n",
    "            cell_fw = tf.contrib.rnn.PhasedLSTMCell(output_size)\n",
    "            cell_bw = tf.contrib.rnn.PhasedLSTMCell(output_size)\n",
    "        else: #LSTM cell\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if num_layers > 1:\n",
    "            cell_fw = tf.nn.rnn_cell.MultiRNNCell([cell_fw] * num_layers)\n",
    "            cell_bw = tf.nn.rnn_cell.MultiRNNCell([cell_bw] * num_layers)\n",
    "\n",
    "        if drop_keep_prob != 1.0:\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell=cell_fw, output_keep_prob=drop_keep_prob)\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell=cell_bw, output_keep_prob=drop_keep_prob)\n",
    "\n",
    "        if skip_connections == True:\n",
    "            cell_fw = tf.contrib.rnn.ResidualWrapper(cell_fw)\n",
    "            cell_bw = tf.contrib.rnn.ResidualWrapper(cell_bw)\n",
    "\n",
    "        if attention == True:\n",
    "            cell_fw = tf.contrib.rnn.AttentionCellWrapper(cell_fw, attn_length=10)\n",
    "            cell_bw = tf.contrib.rnn.AttentionCellWrapper(cell_bw, attn_length=10)\n",
    "\n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            inputs,\n",
    "            sequence_length=lengths,\n",
    "            initial_state_fw=contexts[0],\n",
    "            initial_state_bw=contexts[1],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # ( (outputs_fw,outputs_bw) , (output_state_fw,output_state_bw) )\n",
    "        # in case LSTMCell: output_state_fw = (c_fw,h_fw), and output_state_bw = (c_bw,h_bw)\n",
    "        # each [batch_size x max_seq_length x output_size]\n",
    "        return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we have now added options for different cells, for multiple layers, for dropout, skip connections and word by word attention. All those are tricks of the trade to achieve better performance. We have also expanded the arguments of the `tf.nn.bidirectional_dynamic_rnn()` function such that we can control the initialisation of the RNNs (`initial_state_fw, initial_state_fw`).\n",
    "\n",
    "Now that we've defined an RNN, we can use that to define a first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_tweet_reader(placeholders, label_size, vocab_size, emb_init=None, **options):\n",
    "    emb_dim = options[\"emb_dim\"] # embedding dimensionality\n",
    "\n",
    "    # [batch_size, max_seq1_length]\n",
    "    seq1 = placeholders['tweets']\n",
    "\n",
    "    # [batch_size, labels_size]\n",
    "    labels = tf.to_float(placeholders['labels'])\n",
    "\n",
    "    init = tf.contrib.layers.xavier_initializer(uniform=True)\n",
    "    if init is None:\n",
    "        emb_init = init\n",
    "\n",
    "    # embed the words, i.e. look up the embedding for each word\n",
    "    with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE):\n",
    "        embeddings = tf.get_variable(\"word_embeddings\", [vocab_size, emb_dim], dtype=tf.float32, initializer=emb_init)\n",
    "\n",
    "    with tf.variable_scope(\"embedders\", reuse=tf.AUTO_REUSE) as varscope:\n",
    "        seq1_embedded = tf.nn.embedding_lookup(embeddings, seq1)\n",
    "\n",
    "    # give those embeddings as an input to the RNN reader we have defined above\n",
    "    with tf.variable_scope(\"reader_seq\", reuse=tf.AUTO_REUSE) as varscope1:\n",
    "        # seq1_states: (c_fw, h_fw), (c_bw, h_bw)\n",
    "        outputs, states = reader(seq1_embedded, placeholders['tweets_lengths'], emb_dim,\n",
    "                            scope=varscope1, **options)\n",
    "\n",
    "    # shape output: [batch_size, 2*emb_dim]\n",
    "    if options[\"main_num_layers\"] == 1:\n",
    "        # shape states: [2, 2]\n",
    "        output = tf.concat([states[0][1], states[1][1]], 1)\n",
    "    else:\n",
    "        # shape states: [2, num_layers, 2]\n",
    "        output = tf.concat([states[0][-1][1], states[1][-1][1]], 1)\n",
    "\n",
    "    # pass the RNN encoding to an output layer to make prediction\n",
    "    with tf.variable_scope(\"bilstm_preds\", reuse=tf.AUTO_REUSE):\n",
    "        # output of sequence encoders is projected into an output layer\n",
    "        scores = tf.contrib.layers.fully_connected(output, label_size, weights_initializer=init, activation_fn=tf.tanh)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=scores, labels=labels)\n",
    "        predict = tf.nn.softmax(scores)\n",
    "\n",
    "    return scores, loss, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first model encodes only the tweets using an RNN and makes a prediction based on that encoding. The model consists of three parts: 1) word embedding learning and lookup, 2) tweet encoding with an RNN, 3) output layer: projection of the tweet RNN encoding into the space of output labels\n",
    "\n",
    "**Exercise**: write a variant of the above model that encodes both the tweet and the tweet target with an RNN each.\n",
    "\n",
    "**Thought exercise**: what happens with the word embeddings here and how does it relate to what we have seen in the previous part of the tutorial? Could we use the embeddings we have trained in the previous part for our model? What would be the benefits, downsides and challenges with that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need is a training loop. What we want to do is: for a number of epochs, draw a batch of training instances, train our model on that, adjust the parameters of the model; and repeat this for a fixed number of epochs, or until the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(placeholders, train_feed_dicts, min_op, logits, loss, preds, sess, **options):\n",
    "\n",
    "    max_epochs = options[\"max_epochs\"]\n",
    "\n",
    "    for i in range(1, max_epochs + 1):\n",
    "        loss_all, correct_all = [], 0.0\n",
    "        total, correct_dev_all = 0.0, 0.0\n",
    "        for batch in train_feed_dicts:\n",
    "            _, current_loss, p = sess.run([min_op, loss, preds], feed_dict=batch)\n",
    "            loss_all.append(current_loss)\n",
    "            correct_all, total = calculate_hits(correct_all, total, placeholders, p, batch)\n",
    "\n",
    "        # Randomise batch IDs, so that selection of batch is random\n",
    "        np.random.shuffle(train_feed_dicts)\n",
    "        acc = correct_all / total\n",
    "\n",
    "        mean_loss = np.mean(loss_all)\n",
    "        print('Epoch %d :' % i, \"Loss: \", mean_loss, \"Acc: \", acc)\n",
    "\n",
    "    return logits, loss, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the rest of the training procedure. We have a model and a training loop, now we also need to define an optmiser and we need to initialise our graph. For the optmiser, we could use vanilla gradient descent, or something cleverer that adjusts the learning rate. Here, we use `RMSProp`, which works well in practice and is space-efficient.\n",
    "For ease of use, we wrap this all inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ex4_rnns.classifier_rnns import bicond_reader\n",
    "\n",
    "def train(placeholders, target_labels, train_feed_dicts, vocab, w2v_model=None, sess=None, **options):\n",
    "    # placeholders, labels, data_train, vocab, sess=sess, **options\n",
    "\n",
    "    init = None\n",
    "    if w2v_model != None:\n",
    "        init = tf.constant_initializer(w2v_model.wv.syn0)\n",
    "\n",
    "    # Create model. The second one is the one defined above, the first one encodes both the tweet and the target\n",
    "    if options[\"model_type\"] == 'bicond':\n",
    "        logits, loss, preds = bicond_reader(placeholders, len(target_labels), len(vocab), init, **options)  # those return dicts where the keys are the task names\n",
    "    elif options[\"model_type\"] == 'tweet-only-lstm':\n",
    "        logits, loss, preds = bilstm_tweet_reader(placeholders, len(target_labels), len(vocab), init, **options)  # those return dicts where the keys are the task names\n",
    "\n",
    "    # define an optimiser and initialise graph\n",
    "    optim = tf.train.RMSPropOptimizer(learning_rate=options[\"learning_rate\"])\n",
    "    min_op = optim.minimize(tf.reduce_mean(loss))\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "    # call the training loop function\n",
    "    logits, loss, preds = training_loop(placeholders, train_feed_dicts, min_op, logits, loss, preds, sess, **options)\n",
    "\n",
    "    return logits, loss, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor how well we do during training, we calculate accuracy, in addition to printing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hits(correct_all, total, placeholders, p, batch):\n",
    "    hits = [pp for ii, pp in enumerate(p) if np.argmax(pp) == np.argmax(batch[placeholders[\"targets\"]][ii])]\n",
    "    correct_all += len(hits)\n",
    "    total += len(batch[placeholders[\"targets\"]])\n",
    "    return correct_all, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined everything we need to start training a model. To do this, we need to start a new session, then call the training routine. We first train on the training data, monitoring performance as we go, then apply the trained model to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not take up all the GPU memory all the time.\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    logits, loss, preds = train(placeholders, labels, data_train, vocab, sess=sess, **options)\n",
    "\n",
    "    print(\"Finished training, evaluating on test set\")\n",
    "\n",
    "    correct_test_all, total_test = 0.0, 0.0\n",
    "    p_inds_test, g_inds_test = [], []\n",
    "    for batch_test in data_test:\n",
    "        p_test = sess.run(preds, feed_dict=batch_test)\n",
    "\n",
    "        pred_inds_test = [np.argmax(pp_test) for pp_test in p_test]\n",
    "        p_inds_test.extend(pred_inds_test)\n",
    "        gold_inds_test = [np.argmax(batch_test[placeholders[\"targets\"]][i_d]) for i_d, targ in\n",
    "                              enumerate(batch_test[placeholders[\"targets\"]])]\n",
    "        g_inds_test.extend(gold_inds_test)\n",
    "\n",
    "        correct_test_all, total_test = calculate_hits(correct_test_all, total_test, placeholders, p_test, batch_test)\n",
    "\n",
    "\n",
    "    acc_test = correct_test_all / total_test\n",
    "\n",
    "    print(\"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "- We have defined a number of hyperparameters above, mainly set to allow for rapid prototyping, not to achieve a good performance. What would be better ones? Try a few different combinations and monitor loss, training accuracy and observe test accuracy.\n",
    "- There are also a number of different optmisers you can use, see https://www.tensorflow.org/api_docs/python/tf/train/\n",
    "- Debug tip: if you receive a weird Tensorflow message about reusing variables, select \"`Kernel -> Restart & Run All`\"\n",
    "- Replace the accuracy printing function with the sklearn classification report printing, as introduced in the second part of the tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
