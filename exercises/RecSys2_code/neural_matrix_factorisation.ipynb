{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article Recommendation based on Neural Matrix Factorisation\n",
    "\n",
    "This is an implementation for a latent feature model for recommender systems.\n",
    "The latent feature model F which measures the compatility between a user u and an item m is defined as the dot\n",
    "product of the two latent feature representations of the user and the item, named a and v, respectively, each of size K^F,\n",
    "so a_u for user u and v_m for item m.\n",
    "\n",
    "The feature function is then defined as:\n",
    "\n",
    "theta_{a,v}^F := sum_k^K (a_{u,k} * v_{m,k})\n",
    "\n",
    "For more information on the model, see http://www.aclweb.org/anthology/N13-1008, specially Section 2.1 where the model\n",
    "is defined, and Section 2.5.1 where the BPR loss is defined.\n",
    "\n",
    "The folder also contains a small data snippet for news recommendation in Norwegian. The full dataset can be found here: http://reclab.idi.ntnu.no/dataset/ and is described in the following paper:\n",
    "\n",
    "Gulla, J. A., Zhang, L., Liu, P., Özgöbek, Ö., & Su, X. (2017, August). The Adressa dataset for news recommendation. In Proceedings of the International Conference on Web Intelligence (pp. 1042-1048). ACM.\n",
    "\n",
    "To run the code, you will need to install:\n",
    "- Tensorflow version 1+\n",
    "- numpy\n",
    "\n",
    "After every run, you need to click \"Kernel -> Restart & Run All\", otherwise you will get a Tensorflow error about variable sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some data data preprocessing code; skim briefly so you roughly understand what's going on. Some particularly irrelevant parts are hidden from you and can be found in external Python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "import os\n",
    "from preproc.map import *\n",
    "from preproc.batch import *\n",
    "\n",
    "def read_filtered_records():\n",
    "    # read filtered records and split into positive and negative training data\n",
    "    # only reading some of the data, comment out lines 13 and 14 to use all the data\n",
    "    instances = {\"userID\": [], \"keywords\": [], \"activeTime\": []}\n",
    "            \n",
    "    with open(os.path.join(\"data_selected/\", \"selectedRecords.txt\"), 'r', encoding = 'utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            if i == 100:  # for debugging, to make it run faster\n",
    "                break\n",
    "\n",
    "            obj_userId, obj_activeTime, obj_keywords = line.strip(\" \").strip(\"\\n\").split(\"\\t\")\n",
    "            \n",
    "            print(line.strip(\"\\n\"))\n",
    "\n",
    "            if obj_userId in instances[\"userID\"]:\n",
    "                ind = instances[\"userID\"].index(obj_userId)\n",
    "                ind_i = bisect(instances[\"activeTime\"][ind], obj_activeTime)\n",
    "                instances[\"activeTime\"][ind].insert(ind_i, obj_activeTime)\n",
    "                instances[\"keywords\"][ind].insert(ind_i, obj_keywords)\n",
    "\n",
    "            else:\n",
    "                instances[\"userID\"].append(obj_userId)\n",
    "                instances[\"keywords\"].append([obj_keywords])\n",
    "                instances[\"activeTime\"].append([obj_activeTime])\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        fin.close()\n",
    "        \n",
    "    data = {\"keywords\": [], \"userids\": [], \"targets\": []}\n",
    "    \n",
    "    # Here, the positive and negative instances are constructed based how long the user spends on a web page.\n",
    "    # Assumption: articles with the longest reading time are positive instances, \n",
    "    # and those with the shortest reading time are negative instances\n",
    "    for i, uID in enumerate(instances[\"userID\"]):\n",
    "        # if they've only read one article, it's hard to guess if they liked it or not based on their reading time,\n",
    "        # so we skip it\n",
    "        num_entries = len(instances[\"activeTime\"][i])\n",
    "        if num_entries == 1:\n",
    "            continue\n",
    "            \n",
    "        # sort active time and other data in ascending order\n",
    "        sort_ids = np.argsort([int(k) for k in instances[\"activeTime\"][i]])\n",
    "        active_time_sorted = [instances[\"activeTime\"][i][j] for j in sort_ids]\n",
    "        keys_sorted = [instances[\"keywords\"][i][j] for j in sort_ids]\n",
    "        \n",
    "        #print(instances[\"activeTime\"][i])\n",
    "        #print(active_time_sorted)\n",
    "        #print()\n",
    "            \n",
    "        # negative instances first 10% of articles in terms of reading time\n",
    "        end_index = int((num_entries/10)+1)\n",
    "        keyl = []\n",
    "        for inst in keys_sorted[0:end_index]:\n",
    "            keyl.extend(inst.split(\",\"))\n",
    "        \n",
    "        data[\"keywords\"].append(keyl)\n",
    "        data[\"targets\"].append(1)\n",
    "        data[\"userids\"].append(uID)\n",
    "        \n",
    "        print(uID, \"0\", keyl)\n",
    "        \n",
    "        # positive instances -- last 10% of articles in terms of reading time\n",
    "        start_index = int((num_entries-(num_entries / 10)))\n",
    "        \n",
    "        # negative instances\n",
    "        keyl = []\n",
    "        for inst in keys_sorted[start_index:]:\n",
    "            keyl.extend(inst.split(\",\"))\n",
    "        data[\"keywords\"].append(keyl)\n",
    "        data[\"targets\"].append(0)\n",
    "        data[\"userids\"].append(uID)\n",
    "        \n",
    "        print(uID, \"1\", keyl)\n",
    "        print(\"\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_data(placeholders, data, vocab_keys=None, vocab_users=None):\n",
    "    if 'keywords' in placeholders.keys():\n",
    "        data = deep_seq_map(data, lower, ['keywords'])\n",
    "    if vocab_keys is None:\n",
    "        vocab_keys = Vocab()\n",
    "        vocab_users = Vocab()\n",
    "        if 'keywords' in placeholders.keys():\n",
    "            for instance in data[\"keywords\"]:\n",
    "                for token in instance:\n",
    "                    vocab_keys(token)\n",
    "        for instance in data[\"userids\"]:\n",
    "            vocab_users(instance)\n",
    "\n",
    "    if 'keywords' in placeholders.keys():\n",
    "        data = deep_map(data, vocab_keys, [\"keywords\"])\n",
    "\n",
    "    data = deep_map(data, vocab_users, [\"userids\"])\n",
    "\n",
    "    # removing data that's not a placeholder\n",
    "    popl = []\n",
    "    for k in data.keys():\n",
    "        if not k in placeholders.keys():\n",
    "            popl.append(k)\n",
    "    for p in popl:\n",
    "        data.pop(p, None)\n",
    "\n",
    "    return data, vocab_keys, vocab_users\n",
    "\n",
    "\n",
    "def get_feed_dicts(data_train_np, placeholders, batch_size, inst_length):\n",
    "    data_train_batched = []\n",
    "    realsamp = int(inst_length/batch_size)\n",
    "    additionsamp = batch_size-(inst_length%batch_size)\n",
    "    if additionsamp != 0:\n",
    "        realsamp += 1\n",
    "\n",
    "    # sample so that there are an equal number of pos and neg instances per batch\n",
    "    ids_pos = [i for i in range(0, inst_length) if data_train_np[\"targets\"][i] == 1]\n",
    "    ids_neg = [i for i in range(0, inst_length) if data_train_np[\"targets\"][i] == 0]\n",
    "    ids1_pos = choice(ids_pos, len(ids_pos), replace=False)\n",
    "    ids1_neg = choice(ids_neg, len(ids_neg), replace=False)\n",
    "    ids1 = [val for pair in zip(ids1_pos, ids1_neg) for val in pair]\n",
    "\n",
    "    ids = ids1\n",
    "\n",
    "    start = 0\n",
    "    for i in range(0, realsamp):\n",
    "        batch_i = {}\n",
    "        if i != 0:\n",
    "            start = i * batch_size\n",
    "        if i != realsamp:\n",
    "            ids_sup = ids[start:((i+1)*batch_size)]\n",
    "        else:\n",
    "            ids_sup = ids[start:realsamp]\n",
    "        for key, value in data_train_np.items():\n",
    "            batch_i[placeholders[key]] = [data_train_np[key][ii] for ii in ids_sup]\n",
    "\n",
    "        data_train_batched.append(batch_i)\n",
    "\n",
    "    return data_train_batched\n",
    "\n",
    "\n",
    "def load_data(placeholders, batch_size=8):\n",
    "\n",
    "    data = read_filtered_records()\n",
    "    prepared_data, vocab_keys, vocab_users = prepare_data(placeholders, data)\n",
    "    vocab_keys.freeze()  # this makes sure that nothing further is added to the vocab, otherwise deep_map will extend it\n",
    "    vocab_users.freeze()\n",
    "    numpified_data = numpify(prepared_data, pad=0)\n",
    "    feed_dicts = get_feed_dicts(numpified_data, placeholders, batch_size, len(data[\"userids\"]))\n",
    "    return feed_dicts, vocab_keys, vocab_users\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the important part, the neural matrix factorisation model. Read carefully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def mf_reader(placeholders, vocab_size_keys, vocab_size_users, emb_dim, fact_loss='BPR', lambda_L2 = 0.01):\n",
    "\n",
    "    # [batch_size]\n",
    "    userids = placeholders['userids']\n",
    "\n",
    "    # [batch_size, max_keyw_length]\n",
    "    keywords = placeholders['keywords']\n",
    "\n",
    "    # [batch_size]\n",
    "    targets = tf.to_float(placeholders['targets'])\n",
    "\n",
    "    init = tf.contrib.layers.xavier_initializer(uniform=True)\n",
    "\n",
    "    with tf.variable_scope(\"u_embeddings\", reuse=tf.AUTO_REUSE):\n",
    "        user_embeddings = tf.get_variable(\"user_embeddings\", [vocab_size_users, emb_dim], dtype=tf.float32, initializer=init)\n",
    "\n",
    "    # separate embeddings for users and keywords\n",
    "    with tf.variable_scope(\"k_embeddings\", reuse=tf.AUTO_REUSE):\n",
    "        key_embeddings = tf.get_variable(\"keyw_embeddings\", [vocab_size_keys, emb_dim], dtype=tf.float32, initializer=init)\n",
    "\n",
    "    with tf.variable_scope(\"keyw_embedders\") as varscope:\n",
    "        keyw_embedded = tf.nn.embedding_lookup(key_embeddings, keywords)\n",
    "\n",
    "    with tf.variable_scope(\"uid_embedders\") as varscope:\n",
    "        uid_embedded = tf.nn.embedding_lookup(user_embeddings, userids)\n",
    "\n",
    "    where0 = tf.not_equal(targets, 0)\n",
    "    where1 = tf.not_equal(targets, 1)\n",
    "    user_embeddings_neg = tf.boolean_mask(uid_embedded, where0)\n",
    "    user_embeddings_pos = tf.boolean_mask(uid_embedded, where1)\n",
    "    key_embeddings_neg = tf.boolean_mask(keyw_embedded, where0)\n",
    "    key_embeddings_pos = tf.boolean_mask(keyw_embedded, where1)\n",
    "\n",
    "    key_embeddings_pos = tf.nn.sigmoid(key_embeddings_pos)\n",
    "    key_embeddings_neg = tf.nn.sigmoid(key_embeddings_neg)\n",
    "\n",
    "    # positive and negative dot products\n",
    "    # note that we have a separate embedding for every keyword, so we need to mean average across keywords to get one feature vector for all keywords together\n",
    "    # for predictions (to be ranked)\n",
    "    dotprod_pos = tf.reduce_sum(tf.reduce_sum(tf.multiply(key_embeddings_pos, tf.expand_dims(user_embeddings_pos, 1)), 1), 1)  \n",
    "    \n",
    "    # calculate normalised scores of positive examples\n",
    "    sigma_dotprod_pos = tf.nn.sigmoid(dotprod_pos) \n",
    "    \n",
    "    dotprod_neg = tf.reduce_sum(tf.reduce_sum(tf.multiply(key_embeddings_neg, tf.expand_dims(user_embeddings_neg, 1)), 1), 1)\n",
    "\n",
    "    # calculate normalised scores of negative examples\n",
    "    sigma_dotprod_neg = tf.nn.sigmoid(dotprod_neg)  \n",
    "    \n",
    "    diff_dotprod = tf.reduce_sum(tf.reduce_sum(tf.multiply(key_embeddings_pos, tf.expand_dims(user_embeddings_pos, 1)) - tf.multiply(key_embeddings_neg, tf.expand_dims(user_embeddings_neg, 1)), 1), 1)\n",
    "\n",
    "    if fact_loss == 'logistic':\n",
    "        loss_R = tf.reduce_sum(tf.nn.softplus(-dotprod_pos) + tf.nn.softplus(dotprod_neg))\n",
    "    elif fact_loss == 'BPR':\n",
    "        loss_R = tf.reduce_sum(tf.nn.softplus(-dotprod_pos))\n",
    "        # TO IMPLEMENT!!!\n",
    "\n",
    "    # L2 regularisation\n",
    "    loss_L2_pos = tf.add(tf.nn.l2_loss(user_embeddings_pos), tf.nn.l2_loss(key_embeddings_pos))\n",
    "    loss_L2_neg = tf.add(tf.nn.l2_loss(user_embeddings_neg), tf.nn.l2_loss(key_embeddings_neg))\n",
    "\n",
    "    loss_L2 = tf.add(loss_L2_pos, loss_L2_neg)\n",
    "\n",
    "    \"\"\"total loss\"\"\"\n",
    "    loss = tf.stack([loss_R, tf.scalar_mul(lambda_L2, loss_L2)])\n",
    "\n",
    "    return sigma_dotprod_pos, sigma_dotprod_neg, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the main training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_pl = tf.placeholder(tf.int32, [None, None], name=\"keywords\")\n",
    "userids_pl = tf.placeholder(tf.int32, [None], name=\"userids\")\n",
    "targets = tf.placeholder(tf.int32, [None], name=\"targets\")\n",
    "\n",
    "def create_placeholders():\n",
    "    placeholders = {\"keywords\": keywords_pl, \"userids\": userids_pl, \"targets\": targets}\n",
    "    return placeholders\n",
    "\n",
    "def main(model_variant=\"similarity\", max_epochs=500):\n",
    "\n",
    "    placeholders = create_placeholders()\n",
    "\n",
    "    feed_dicts, vocab_keys, vocab_users = load_data(placeholders, batch_size=8)\n",
    "\n",
    "    # Do not take up all the GPU memory all the time.\n",
    "    sess_config = tf.ConfigProto()\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "\n",
    "        # create model\n",
    "        dotprod_pos, dotprod_neg, loss = mf_reader(placeholders, len(vocab_keys), len(vocab_users), emb_dim=32)\n",
    "\n",
    "        optim = tf.train.RMSPropOptimizer(learning_rate=0.0005)\n",
    "        min_op = optim.minimize(tf.reduce_mean(loss))\n",
    "\n",
    "        tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "        for i in range(1, max_epochs + 1):\n",
    "            loss_all, correct_all, correct_all_neg, total = [], 0.0, 0.0, 0.0\n",
    "            for j in range(0, len(feed_dicts)):\n",
    "                batch = feed_dicts[j]\n",
    "                # we get the predictions for positive and negative instances on the training data\n",
    "                _, current_loss, p_pos = sess.run([min_op, loss, dotprod_pos], feed_dict=batch)\n",
    "\n",
    "                # to test on test data, this would need to be split off the training data\n",
    "                # and the following line(s) would need to be run - eval as below.\n",
    "                # p_test_pos = sess.run(dotprod_pos, feed_dict=batch_test)  # for positive instances\n",
    "\n",
    "                # as for training data, each test batch needs to contain an equal number of positive and negative samples\n",
    "                p_neg = sess.run(dotprod_neg, feed_dict=batch)  # for negative instances\n",
    "\n",
    "                loss_all.extend(current_loss)\n",
    "                preds = np.round(p_pos)\n",
    "                hits_pos = [pp for pp in preds if pp == 1.0]  # we only care about performing well for positive instances\n",
    "                preds_neg = np.round(p_neg)\n",
    "                hits_neg = [pp for pp in preds_neg if pp == 0.0]\n",
    "                correct_all += len(hits_pos)\n",
    "                correct_all_neg += len(hits_neg)\n",
    "                total += len(preds)\n",
    "\n",
    "            # Randomise batch IDs, so that selection of batch is random\n",
    "            np.random.shuffle(feed_dicts)\n",
    "            acc = correct_all / total\n",
    "            acc_neg = correct_all_neg / total\n",
    "            print('Epoch %d :' % i, \"Acc Train Pos: \", acc, \"Acc Train Neg: \", acc_neg, \"Loss: \", np.mean(loss_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cx:lo0s6rngjd111p7dgxxvj2y6y:33hmfzes9x0yn\t9\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:hzx2pn6j7vhtjyng:sckfhjlv4pdz\t466\telbil,Trondheim parkering,sissel trønsdal,Trondheim\n",
      "cx:2gcnj061e2zz1dwnqk8yhv2yp:2r0fe9ekqec1v\t32\tVassfjellet,Innbrudd\n",
      "cx:ingb68nkjywbce3d:3aizgkitiquss\t331\tBaby,Bioteknologi,Fødsel,Helse,Svangerskap\n",
      "cx:ifa61e6grusoket6:3nbl45cgx3lz7\t134\tTrøndelag politidistrikt\n",
      "cx:hwcel2lt7o01whx8:3m828rocxjbne\t51\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:1wuel58my406c277lpq0k7gmy4:39vwjf2pw5138\t21\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:fz50mkepskbz1bhbsonrxatwb:2bv2dnsix8gwl\t114\tVassfjellet,Innbrudd\n",
      "cx:ilhmumvrt41eo6wb:1uhvfttq77gh8\t46\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:ikt31ks11f0v988r:mgiyccdm870o\t91\tHund,Ulv,Verdal\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a\t48\tVassfjellet,Innbrudd\n",
      "cx:ijrnuuor7mbrohmg:1ey4i9ki64xt\t15\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:y59y853q1amo3am73byk7angz:1ugpwe02cp5kx\t91\tVassfjellet,Innbrudd\n",
      "cx:ijphgcgc3cbaumob:cnb630ck60ex\t82\tHund,Ulv,Verdal\n",
      "cx:3fjx3k6a1nkwc290gomyz1mqmm:18h96zd873dvo\t75\tHund,Ulv,Verdal\n",
      "cx:2yoypezxan5xy1qrwmifsk2iqe:23i2d478h82dd\t43\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ik9s5jbx6j637hae:20hjr5f2scl13\t31\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:2w8ltvm2r8dc35f8cuj1lwa5o:3f7p3yo3ahp44\t74\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:ijrnuuor7mbrohmg:1ey4i9ki64xt\t22\telbil,Trondheim parkering,sissel trønsdal,Trondheim\n",
      "cx:387c1v8rng742200p4ht8br3ry:wz439aj8puft\t11\tNTNU,NTNU campus,Gunnar Bovim,Rita Ottervik,Trond Giske,Sivert Bjørnstad\n",
      "cx:iha8h2vann7kqtnz:1q5g23tm3j10h\t37\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ijep25nkd7itprbt:3uobjxcc1eysw\t145\tHund,Ulv,Verdal\n",
      "cx:2gcnj061e2zz1dwnqk8yhv2yp:2r0fe9ekqec1v\t135\tHybridbil,Elbil\n",
      "cx:1rfwmyl8199aw2sqagjs6kspnz:2vn4bfrm61vyt\t131\tHund,Ulv,Verdal\n",
      "cx:1l2z7rjbxfyn93opo6x66dw9xq:3pxqdt16f67ca\t20\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:2zjasdcgvn6t7dbngtpnoddsr:2xwzi0yh6odye\t23\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:in6kmjalp6byw41d:1dwbvi7ggavd6\t174\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:ijrnuuor7mbrohmg:1ey4i9ki64xt\t1\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:387c1v8rng742200p4ht8br3ry:wz439aj8puft\t35\tVassfjellet,Innbrudd\n",
      "cx:3gw2nq96dpd512xxmxj7wg861y:3co84d1esin5h\t25\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ik05mep34jwo9oai:uqhohl8zg3l4\t3\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:vk15lqgee6envhwzs8mlkcyy:1wgep3b8gb2ps\t11\tmobil\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a\t3\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:2zjasdcgvn6t7dbngtpnoddsr:2xwzi0yh6odye\t89\tHund,Ulv,Verdal\n",
      "cx:ilhmumvrt41eo6wb:1uhvfttq77gh8\t27\tTrøndelag politidistrikt\n",
      "cx:373kh1pm9636g2z6hx00wekbi6:3g62kmpu2owvl\t106\tVassfjellet,Innbrudd\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a\t15\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:igv1gdc9evybjnpi:38kuzlqn6xt9u\t115\tTrøndelag politidistrikt\n",
      "cx:ie61ga8udk5w0eb9:25o6jxwofhyq9\t183\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:im33cr5t9vh4ylwe:2y87w442pc2k6\t300\telbil,Trondheim parkering,sissel trønsdal,Trondheim\n",
      "cx:1md9tlyft7puk1sxf2y1n5jhh3:3iqvjco0i1l02\t83\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:inkdbu74kyi5jyuk:1vthzvjeyvhtf\t8\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:iha8h2vann7kqtnz:1q5g23tm3j10h\t82\tHund,Ulv,Verdal\n",
      "cx:ig1zh6lz2md204nx:3a7og4crsp2bs\t70\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:3gw2nq96dpd512xxmxj7wg861y:3co84d1esin5h\t67\telbil,Trondheim parkering,sissel trønsdal,Trondheim\n",
      "cx:i5jhonpmftas8hsa:2fjd8jpzhate7\t197\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:ibawlzf8k8o9k4mc:ksw3zrf15wku\t235\tHopp\n",
      "cx:ik9s5jbx6j637hae:20hjr5f2scl13\t120\ttog,NSB,Samferdsel,Reiseliv,debatt\n",
      "cx:i1nh7nuboyq02nd9:2vqrqng5zztsx\t206\tmatematikk,Lærerløftet,Lærerutdanning,NTNU\n",
      "cx:21riktlcqg8bxasesrcleft8f:3b9evi7tygq70\t29\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ijqw96gwchjjasgn:3kv58ltz71k2b\t34\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:1359737723239887528960:avfcoljrgl5u\t31\tRema,Dagligvare,Økonomi\n",
      "cx:1l2z7rjbxfyn93opo6x66dw9xq:3pxqdt16f67ca\t124\tkamskjelldykkerne,Politiet,arbeidsmiljø,Arbeidsulykke,Frøya\n",
      "cx:1zlvtt2rf1eckwzgur10pl8ij:13kvzen7c6dud\t2\tTrøndelag politidistrikt\n",
      "cx:hwcel2lt7o01whx8:3m828rocxjbne\t36\tHybridbil,Elbil\n",
      "cx:2dhhv3dz6lvbh3cx9fnfkbvybs:1igq2p9ozhel8\t68\tmatematikk,Lærerløftet,Lærerutdanning,NTNU\n",
      "cx:htkipe5w759w5vw9:2souxtezokfus\t63\tHund,Ulv,Verdal\n",
      "cx:ijvsp5lb5xytj8aw:1ppyu2mwk28jv\t4\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:iiu62w2vloz31sr4:21asif8ezms3c\t7\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:iodc1bb2ynq1zvb0:rumsjef3oy5p\t42\tHund,Ulv,Verdal\n",
      "cx:htc3b8wlcmdz53w0:2o9n04x2y9jp8\t13\tTour de ski,Ski,Charlotte Kalla\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a\t13\tkamskjelldykkerne,Politiet,arbeidsmiljø,Arbeidsulykke,Frøya\n",
      "cx:3pwek0xebbcob2a35hwj2n0b58:16evss0o4i4vv\t225\tÅge Aleksandersen,Musikk,Byåsen,Kirken,Folk,Gunnar Pedersen,Royal Albert Hall\n",
      "cx:1314301665754761626909:1578oj8o9m8xn\t58\tRanheim IL,OBOS-ligaen,Fotball 1. div,Eliteserien,Fotball,Molde FK\n",
      "cx:ik05mep34jwo9oai:uqhohl8zg3l4\t7\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:inkdbu74kyi5jyuk:1vthzvjeyvhtf\t10\tHund,Ulv,Verdal\n",
      "cx:3810dwwu80yre1ucn64o67tdu9:2pamvywvjxwoa\t137\tTrening,Overvekt,Næringsmiddelindustrien\n",
      "cx:ia2hhlmgmiqn5mm3:defcmikz7aw7\t116\tHopp,Farger,Daniel Andre Tande\n",
      "cx:2yoypezxan5xy1qrwmifsk2iqe:23i2d478h82dd\t92\tHund,Ulv,Verdal\n",
      "cx:c8dd1d33179b0fbce7b305539826f30a:3g24z6t0ijhi\t31\tHund,Ulv,Verdal\n",
      "cx:1zlvtt2rf1eckwzgur10pl8ij:13kvzen7c6dud\t17\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:2zeq8qbe3ayv321t7puv5lwcpb:2ya68s7i3vppj\t22\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:2mf7o9i8pfxqiesue3lgfn203:2qillayddul8f\t41\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:3fjx3k6a1nkwc290gomyz1mqmm:18h96zd873dvo\t92\telbil,Trondheim parkering,sissel trønsdal,Trondheim\n",
      "cx:ilhmumvrt41eo6wb:1uhvfttq77gh8\t12\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:inkdbu74kyi5jyuk:1vthzvjeyvhtf\t15\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ijphgcgc3cbaumob:cnb630ck60ex\t128\tVassfjellet,Innbrudd\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a\t13\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:ijvsp5lb5xytj8aw:1ppyu2mwk28jv\t186\tRema,Dagligvare,Økonomi\n",
      "cx:mr4wc7nwqlme2emjy1dudxa91:1eno25107x9i2\t174\tHund,verdens tøffeste hundeløp,tur\n",
      "cx:1wco29bnwjeb62xv90tupcvo4w:2lq75ceqoddxc\t43\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ibb7ns9j6ejd46rh:8ms7yxl8hp42\t262\tTrening,Overvekt,Næringsmiddelindustrien\n",
      "cx:htc3b8wlcmdz53w0:2o9n04x2y9jp8\t20\tHåndball,Linn Jørum Sulland,Jeanett Kristiansen,Vipers Kristiansand,Kenneth Gabrielsen\n",
      "cx:ilhmumvrt41eo6wb:1uhvfttq77gh8\t16\tRema,Dagligvare,Økonomi\n",
      "cx:3gw2nq96dpd512xxmxj7wg861y:3co84d1esin5h\t25\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:ik05mep34jwo9oai:uqhohl8zg3l4\t21\tTrening,Overvekt,Næringsmiddelindustrien\n",
      "cx:ig1zh6lz2md204nx:3a7og4crsp2bs\t25\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:3bqckvglpns0821uushv269auj:q8h30zm37cr5\t84\tHund,Ulv,Verdal\n",
      "cx:2vzfgneipfago1vr05ebuwg9mk:yv5rf6kdgtlg\t81\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:htkipe5w759w5vw9:2souxtezokfus\t5\tHund,Jakt,hundekjøring,Kjæledyr\n",
      "cx:iodc1bb2ynq1zvb0:rumsjef3oy5p\t68\tpluss,eadressa,ukeadressa,digitalt,arkiv,fordeler\n",
      "cx:io5mlomhadajxeh2:2wdr8prolv29y\t9\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:13721513391821129554703:15tjuaf4pjbst\t56\tStue,Trend,Farger,Hjemme hos,Interiør,Bolig\n",
      "cx:1343562653128429269256:1wz752ffi1771\t24\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:htkipe5w759w5vw9:2souxtezokfus\t32\tHund,Ulv,Verdal\n",
      "cx:ill1g3u6lik8vr3o:1ugzkuk3ut2kj\t56\tutenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker\n",
      "cx:idc4dkrljbviodge:1eggkl2ukpju9\t31\tVassfjellet,Innbrudd\n",
      "cx:ilcshwymctldhhz5:oprv1ke6o8np\t60\tTour de ski,Ski,Petter Northug,Martin Johnsrud Sundby\n",
      "cx:io5mlomhadajxeh2:2wdr8prolv29y\t14\tHybridbil,Elbil\n",
      "cx:iog22jtg9aroizrs:2rfjnaet0ie8e\t50\tSki-VM,Langrenn,Trondheim\n",
      "cx:2gcnj061e2zz1dwnqk8yhv2yp:2r0fe9ekqec1v 0 ['Vassfjellet', 'Innbrudd']\n",
      "cx:2gcnj061e2zz1dwnqk8yhv2yp:2r0fe9ekqec1v 1 ['Hybridbil', 'Elbil']\n",
      "\n",
      "cx:hwcel2lt7o01whx8:3m828rocxjbne 0 ['Hybridbil', 'Elbil']\n",
      "cx:hwcel2lt7o01whx8:3m828rocxjbne 1 ['tog', 'NSB', 'Samferdsel', 'Reiseliv', 'debatt']\n",
      "\n",
      "cx:ilhmumvrt41eo6wb:1uhvfttq77gh8 0 ['Stue', 'Trend', 'Farger', 'Hjemme hos', 'Interiør', 'Bolig']\n",
      "cx:ilhmumvrt41eo6wb:1uhvfttq77gh8 1 ['tog', 'NSB', 'Samferdsel', 'Reiseliv', 'debatt']\n",
      "\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a 0 ['tog', 'NSB', 'Samferdsel', 'Reiseliv', 'debatt']\n",
      "cx:2hlokd1ecthx81gia99yy464xt:13b8ml7zctj4a 1 ['Vassfjellet', 'Innbrudd']\n",
      "\n",
      "cx:ijrnuuor7mbrohmg:1ey4i9ki64xt 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:ijrnuuor7mbrohmg:1ey4i9ki64xt 1 ['elbil', 'Trondheim parkering', 'sissel trønsdal', 'Trondheim']\n",
      "\n",
      "cx:ijphgcgc3cbaumob:cnb630ck60ex 0 ['Hund', 'Ulv', 'Verdal']\n",
      "cx:ijphgcgc3cbaumob:cnb630ck60ex 1 ['Vassfjellet', 'Innbrudd']\n",
      "\n",
      "cx:3fjx3k6a1nkwc290gomyz1mqmm:18h96zd873dvo 0 ['Hund', 'Ulv', 'Verdal']\n",
      "cx:3fjx3k6a1nkwc290gomyz1mqmm:18h96zd873dvo 1 ['elbil', 'Trondheim parkering', 'sissel trønsdal', 'Trondheim']\n",
      "\n",
      "cx:2yoypezxan5xy1qrwmifsk2iqe:23i2d478h82dd 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:2yoypezxan5xy1qrwmifsk2iqe:23i2d478h82dd 1 ['Hund', 'Ulv', 'Verdal']\n",
      "\n",
      "cx:ik9s5jbx6j637hae:20hjr5f2scl13 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:ik9s5jbx6j637hae:20hjr5f2scl13 1 ['tog', 'NSB', 'Samferdsel', 'Reiseliv', 'debatt']\n",
      "\n",
      "cx:387c1v8rng742200p4ht8br3ry:wz439aj8puft 0 ['NTNU', 'NTNU campus', 'Gunnar Bovim', 'Rita Ottervik', 'Trond Giske', 'Sivert Bjørnstad']\n",
      "cx:387c1v8rng742200p4ht8br3ry:wz439aj8puft 1 ['Vassfjellet', 'Innbrudd']\n",
      "\n",
      "cx:iha8h2vann7kqtnz:1q5g23tm3j10h 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:iha8h2vann7kqtnz:1q5g23tm3j10h 1 ['Hund', 'Ulv', 'Verdal']\n",
      "\n",
      "cx:1l2z7rjbxfyn93opo6x66dw9xq:3pxqdt16f67ca 0 ['tog', 'NSB', 'Samferdsel', 'Reiseliv', 'debatt']\n",
      "cx:1l2z7rjbxfyn93opo6x66dw9xq:3pxqdt16f67ca 1 ['kamskjelldykkerne', 'Politiet', 'arbeidsmiljø', 'Arbeidsulykke', 'Frøya']\n",
      "\n",
      "cx:2zjasdcgvn6t7dbngtpnoddsr:2xwzi0yh6odye 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:2zjasdcgvn6t7dbngtpnoddsr:2xwzi0yh6odye 1 ['Hund', 'Ulv', 'Verdal']\n",
      "\n",
      "cx:3gw2nq96dpd512xxmxj7wg861y:3co84d1esin5h 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:3gw2nq96dpd512xxmxj7wg861y:3co84d1esin5h 1 ['elbil', 'Trondheim parkering', 'sissel trønsdal', 'Trondheim']\n",
      "\n",
      "cx:ik05mep34jwo9oai:uqhohl8zg3l4 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:ik05mep34jwo9oai:uqhohl8zg3l4 1 ['Trening', 'Overvekt', 'Næringsmiddelindustrien']\n",
      "\n",
      "cx:inkdbu74kyi5jyuk:1vthzvjeyvhtf 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:inkdbu74kyi5jyuk:1vthzvjeyvhtf 1 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "\n",
      "cx:ig1zh6lz2md204nx:3a7og4crsp2bs 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:ig1zh6lz2md204nx:3a7og4crsp2bs 1 ['tog', 'NSB', 'Samferdsel', 'Reiseliv', 'debatt']\n",
      "\n",
      "cx:1zlvtt2rf1eckwzgur10pl8ij:13kvzen7c6dud 0 ['Trøndelag politidistrikt']\n",
      "cx:1zlvtt2rf1eckwzgur10pl8ij:13kvzen7c6dud 1 ['Stue', 'Trend', 'Farger', 'Hjemme hos', 'Interiør', 'Bolig']\n",
      "\n",
      "cx:htkipe5w759w5vw9:2souxtezokfus 0 ['Hund', 'Jakt', 'hundekjøring', 'Kjæledyr']\n",
      "cx:htkipe5w759w5vw9:2souxtezokfus 1 ['Hund', 'Ulv', 'Verdal']\n",
      "\n",
      "cx:ijvsp5lb5xytj8aw:1ppyu2mwk28jv 0 ['Stue', 'Trend', 'Farger', 'Hjemme hos', 'Interiør', 'Bolig']\n",
      "cx:ijvsp5lb5xytj8aw:1ppyu2mwk28jv 1 ['Rema', 'Dagligvare', 'Økonomi']\n",
      "\n",
      "cx:iodc1bb2ynq1zvb0:rumsjef3oy5p 0 ['Hund', 'Ulv', 'Verdal']\n",
      "cx:iodc1bb2ynq1zvb0:rumsjef3oy5p 1 ['pluss', 'eadressa', 'ukeadressa', 'digitalt', 'arkiv', 'fordeler']\n",
      "\n",
      "cx:htc3b8wlcmdz53w0:2o9n04x2y9jp8 0 ['Tour de ski', 'Ski', 'Charlotte Kalla']\n",
      "cx:htc3b8wlcmdz53w0:2o9n04x2y9jp8 1 ['Håndball', 'Linn Jørum Sulland', 'Jeanett Kristiansen', 'Vipers Kristiansand', 'Kenneth Gabrielsen']\n",
      "\n",
      "cx:io5mlomhadajxeh2:2wdr8prolv29y 0 ['utenriks', 'innenriks', 'trondheim', 'E6', 'midtbyen', 'bybrann', 'bilulykker']\n",
      "cx:io5mlomhadajxeh2:2wdr8prolv29y 1 ['Hybridbil', 'Elbil']\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-3937583eb00d>:13: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Acc Train Pos:  0.43478260869565216 Acc Train Neg:  0.5652173913043478 Loss:  3.7774727\n",
      "Epoch 2 : Acc Train Pos:  0.43478260869565216 Acc Train Neg:  0.5652173913043478 Loss:  3.7005336\n",
      "Epoch 3 : Acc Train Pos:  0.43478260869565216 Acc Train Neg:  0.5652173913043478 Loss:  3.6268413\n",
      "Epoch 4 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.5217391304347826 Loss:  3.5550127\n",
      "Epoch 5 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.5217391304347826 Loss:  3.4853141\n",
      "Epoch 6 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.5217391304347826 Loss:  3.4173307\n",
      "Epoch 7 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.5217391304347826 Loss:  3.349758\n",
      "Epoch 8 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.5217391304347826 Loss:  3.2841322\n",
      "Epoch 9 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.5217391304347826 Loss:  3.2198665\n",
      "Epoch 10 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.4782608695652174 Loss:  3.156209\n",
      "Epoch 11 : Acc Train Pos:  0.4782608695652174 Acc Train Neg:  0.4782608695652174 Loss:  3.0941648\n",
      "Epoch 12 : Acc Train Pos:  0.5217391304347826 Acc Train Neg:  0.43478260869565216 Loss:  3.0329504\n",
      "Epoch 13 : Acc Train Pos:  0.5217391304347826 Acc Train Neg:  0.43478260869565216 Loss:  2.9728622\n",
      "Epoch 14 : Acc Train Pos:  0.5217391304347826 Acc Train Neg:  0.43478260869565216 Loss:  2.9136312\n",
      "Epoch 15 : Acc Train Pos:  0.5217391304347826 Acc Train Neg:  0.43478260869565216 Loss:  2.8560982\n",
      "Epoch 16 : Acc Train Pos:  0.5652173913043478 Acc Train Neg:  0.43478260869565216 Loss:  2.7991636\n",
      "Epoch 17 : Acc Train Pos:  0.5652173913043478 Acc Train Neg:  0.43478260869565216 Loss:  2.7429464\n",
      "Epoch 18 : Acc Train Pos:  0.5652173913043478 Acc Train Neg:  0.43478260869565216 Loss:  2.6881864\n",
      "Epoch 19 : Acc Train Pos:  0.5652173913043478 Acc Train Neg:  0.43478260869565216 Loss:  2.6345484\n",
      "Epoch 20 : Acc Train Pos:  0.6086956521739131 Acc Train Neg:  0.43478260869565216 Loss:  2.5821307\n",
      "Epoch 21 : Acc Train Pos:  0.6086956521739131 Acc Train Neg:  0.391304347826087 Loss:  2.530865\n",
      "Epoch 22 : Acc Train Pos:  0.6086956521739131 Acc Train Neg:  0.391304347826087 Loss:  2.4805932\n",
      "Epoch 23 : Acc Train Pos:  0.6086956521739131 Acc Train Neg:  0.391304347826087 Loss:  2.4315903\n",
      "Epoch 24 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.34782608695652173 Loss:  2.3831244\n",
      "Epoch 25 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.34782608695652173 Loss:  2.336457\n",
      "Epoch 26 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.34782608695652173 Loss:  2.2906218\n",
      "Epoch 27 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.34782608695652173 Loss:  2.2458785\n",
      "Epoch 28 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.34782608695652173 Loss:  2.2026033\n",
      "Epoch 29 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.34782608695652173 Loss:  2.1599357\n",
      "Epoch 30 : Acc Train Pos:  0.6521739130434783 Acc Train Neg:  0.30434782608695654 Loss:  2.1185434\n",
      "Epoch 31 : Acc Train Pos:  0.7391304347826086 Acc Train Neg:  0.2608695652173913 Loss:  2.078306\n",
      "Epoch 32 : Acc Train Pos:  0.7391304347826086 Acc Train Neg:  0.2608695652173913 Loss:  2.0392795\n",
      "Epoch 33 : Acc Train Pos:  0.7391304347826086 Acc Train Neg:  0.2608695652173913 Loss:  2.0016158\n",
      "Epoch 34 : Acc Train Pos:  0.7391304347826086 Acc Train Neg:  0.2608695652173913 Loss:  1.9650154\n",
      "Epoch 35 : Acc Train Pos:  0.7391304347826086 Acc Train Neg:  0.21739130434782608 Loss:  1.929231\n",
      "Epoch 36 : Acc Train Pos:  0.7391304347826086 Acc Train Neg:  0.17391304347826086 Loss:  1.894763\n",
      "Epoch 37 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.8611746\n",
      "Epoch 38 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.8287516\n",
      "Epoch 39 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.7977904\n",
      "Epoch 40 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.767717\n",
      "Epoch 41 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.7387029\n",
      "Epoch 42 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.7105751\n",
      "Epoch 43 : Acc Train Pos:  0.8260869565217391 Acc Train Neg:  0.17391304347826086 Loss:  1.6835123\n",
      "Epoch 44 : Acc Train Pos:  0.8695652173913043 Acc Train Neg:  0.13043478260869565 Loss:  1.657339\n",
      "Epoch 45 : Acc Train Pos:  0.9130434782608695 Acc Train Neg:  0.13043478260869565 Loss:  1.631976\n",
      "Epoch 46 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.13043478260869565 Loss:  1.6072646\n",
      "Epoch 47 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.08695652173913043 Loss:  1.5838561\n",
      "Epoch 48 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.08695652173913043 Loss:  1.5610547\n",
      "Epoch 49 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.08695652173913043 Loss:  1.5393611\n",
      "Epoch 50 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.08695652173913043 Loss:  1.5182276\n",
      "Epoch 51 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.498207\n",
      "Epoch 52 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.4788599\n",
      "Epoch 53 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.4602968\n",
      "Epoch 54 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.4422709\n",
      "Epoch 55 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.4250914\n",
      "Epoch 56 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.4085034\n",
      "Epoch 57 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.392743\n",
      "Epoch 58 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.3775982\n",
      "Epoch 59 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.3630381\n",
      "Epoch 60 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.3491611\n",
      "Epoch 61 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.3358871\n",
      "Epoch 62 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.3233619\n",
      "Epoch 63 : Acc Train Pos:  0.9565217391304348 Acc Train Neg:  0.043478260869565216 Loss:  1.3113074\n",
      "Epoch 64 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.2997352\n",
      "Epoch 65 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.2884389\n",
      "Epoch 66 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.2778426\n",
      "Epoch 67 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.2674985\n",
      "Epoch 68 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.2576538\n",
      "Epoch 69 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.248215\n",
      "Epoch 70 : Acc Train Pos:  1.0 Acc Train Neg:  0.043478260869565216 Loss:  1.2391642\n",
      "Epoch 71 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.2305343\n",
      "Epoch 72 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.2223469\n",
      "Epoch 73 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.2143546\n",
      "Epoch 74 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.2066976\n",
      "Epoch 75 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1994277\n",
      "Epoch 76 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1923574\n",
      "Epoch 77 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1856301\n",
      "Epoch 78 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1791915\n",
      "Epoch 79 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1729354\n",
      "Epoch 80 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1669096\n",
      "Epoch 81 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1610264\n",
      "Epoch 82 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1554166\n",
      "Epoch 83 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1499093\n",
      "Epoch 84 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1446325\n",
      "Epoch 85 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1395001\n",
      "Epoch 86 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1345905\n",
      "Epoch 87 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1297826\n",
      "Epoch 88 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1250592\n",
      "Epoch 89 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1204917\n",
      "Epoch 90 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.116087\n",
      "Epoch 91 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1117734\n",
      "Epoch 92 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1075186\n",
      "Epoch 93 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.1033849\n",
      "Epoch 94 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0993801\n",
      "Epoch 95 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.095452\n",
      "Epoch 96 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0915743\n",
      "Epoch 97 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0878035\n",
      "Epoch 98 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0840883\n",
      "Epoch 99 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.080465\n",
      "Epoch 100 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0768908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0733792\n",
      "Epoch 102 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0699106\n",
      "Epoch 103 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0665085\n",
      "Epoch 104 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0631652\n",
      "Epoch 105 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0598742\n",
      "Epoch 106 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.056624\n",
      "Epoch 107 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0534242\n",
      "Epoch 108 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0502714\n",
      "Epoch 109 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0471718\n",
      "Epoch 110 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0440948\n",
      "Epoch 111 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0410427\n",
      "Epoch 112 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0380324\n",
      "Epoch 113 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0350599\n",
      "Epoch 114 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0321286\n",
      "Epoch 115 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0292294\n",
      "Epoch 116 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0263608\n",
      "Epoch 117 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0235219\n",
      "Epoch 118 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0207075\n",
      "Epoch 119 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0179071\n",
      "Epoch 120 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0151551\n",
      "Epoch 121 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0124288\n",
      "Epoch 122 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0097233\n",
      "Epoch 123 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0070289\n",
      "Epoch 124 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.0043615\n",
      "Epoch 125 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  1.001733\n",
      "Epoch 126 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9991252\n",
      "Epoch 127 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.99652576\n",
      "Epoch 128 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9939347\n",
      "Epoch 129 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.99136156\n",
      "Epoch 130 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.98880213\n",
      "Epoch 131 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9862835\n",
      "Epoch 132 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.983773\n",
      "Epoch 133 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.98126477\n",
      "Epoch 134 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9787781\n",
      "Epoch 135 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9763052\n",
      "Epoch 136 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9738433\n",
      "Epoch 137 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.97140914\n",
      "Epoch 138 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.96899265\n",
      "Epoch 139 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.96658236\n",
      "Epoch 140 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9641661\n",
      "Epoch 141 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.961767\n",
      "Epoch 142 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9593925\n",
      "Epoch 143 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9570329\n",
      "Epoch 144 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9546819\n",
      "Epoch 145 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.952341\n",
      "Epoch 146 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.94999933\n",
      "Epoch 147 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.94765097\n",
      "Epoch 148 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.94533163\n",
      "Epoch 149 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9430268\n",
      "Epoch 150 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.940717\n",
      "Epoch 151 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9384232\n",
      "Epoch 152 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9361563\n",
      "Epoch 153 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9338867\n",
      "Epoch 154 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.93160695\n",
      "Epoch 155 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9293484\n",
      "Epoch 156 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.927104\n",
      "Epoch 157 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.92487067\n",
      "Epoch 158 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9226308\n",
      "Epoch 159 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9204085\n",
      "Epoch 160 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.91818947\n",
      "Epoch 161 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.91597986\n",
      "Epoch 162 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.91377956\n",
      "Epoch 163 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.91158575\n",
      "Epoch 164 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9093811\n",
      "Epoch 165 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9071868\n",
      "Epoch 166 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.9050072\n",
      "Epoch 167 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.90283734\n",
      "Epoch 168 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.90066934\n",
      "Epoch 169 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8985131\n",
      "Epoch 170 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8963532\n",
      "Epoch 171 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8942039\n",
      "Epoch 172 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8920682\n",
      "Epoch 173 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8899341\n",
      "Epoch 174 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.88779837\n",
      "Epoch 175 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.88566303\n",
      "Epoch 176 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8835389\n",
      "Epoch 177 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.88141996\n",
      "Epoch 178 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8793013\n",
      "Epoch 179 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.87718874\n",
      "Epoch 180 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.87508816\n",
      "Epoch 181 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.87300473\n",
      "Epoch 182 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.87092566\n",
      "Epoch 183 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.86883956\n",
      "Epoch 184 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8667539\n",
      "Epoch 185 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8646781\n",
      "Epoch 186 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.86261135\n",
      "Epoch 187 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8605537\n",
      "Epoch 188 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.858496\n",
      "Epoch 189 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8564415\n",
      "Epoch 190 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.85440063\n",
      "Epoch 191 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.85236305\n",
      "Epoch 192 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.85032195\n",
      "Epoch 193 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.84827995\n",
      "Epoch 194 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8462467\n",
      "Epoch 195 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.844224\n",
      "Epoch 196 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8422022\n",
      "Epoch 197 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8401764\n",
      "Epoch 198 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8381531\n",
      "Epoch 199 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8361576\n",
      "Epoch 200 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.83416563\n",
      "Epoch 201 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8321602\n",
      "Epoch 202 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.83016443\n",
      "Epoch 203 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8281761\n",
      "Epoch 204 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.82619005\n",
      "Epoch 205 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.82421035\n",
      "Epoch 206 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8222358\n",
      "Epoch 207 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.82026535\n",
      "Epoch 208 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8182954\n",
      "Epoch 209 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8163285\n",
      "Epoch 210 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.81437683\n",
      "Epoch 211 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.81241655\n",
      "Epoch 212 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.81047297\n",
      "Epoch 213 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.80854386\n",
      "Epoch 214 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.8065977\n",
      "Epoch 215 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.80465126\n",
      "Epoch 216 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.80272484\n",
      "Epoch 217 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.80080175\n",
      "Epoch 218 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.79886746\n",
      "Epoch 219 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.79694\n",
      "Epoch 220 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7950234\n",
      "Epoch 221 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7931122\n",
      "Epoch 222 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.79121494\n",
      "Epoch 223 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7893126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7874062\n",
      "Epoch 225 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7855148\n",
      "Epoch 226 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7836189\n",
      "Epoch 227 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.78172636\n",
      "Epoch 228 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7798516\n",
      "Epoch 229 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7779786\n",
      "Epoch 230 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7761147\n",
      "Epoch 231 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.77425843\n",
      "Epoch 232 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.77240473\n",
      "Epoch 233 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7705459\n",
      "Epoch 234 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.76868767\n",
      "Epoch 235 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.76682085\n",
      "Epoch 236 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.76496214\n",
      "Epoch 237 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7631213\n",
      "Epoch 238 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.76128083\n",
      "Epoch 239 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.75944275\n",
      "Epoch 240 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7576061\n",
      "Epoch 241 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.75578386\n",
      "Epoch 242 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7539704\n",
      "Epoch 243 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.75214505\n",
      "Epoch 244 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.75032586\n",
      "Epoch 245 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.74852306\n",
      "Epoch 246 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7467227\n",
      "Epoch 247 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.744909\n",
      "Epoch 248 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7430946\n",
      "Epoch 249 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7413005\n",
      "Epoch 250 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.73950416\n",
      "Epoch 251 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7377053\n",
      "Epoch 252 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7359182\n",
      "Epoch 253 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.73414654\n",
      "Epoch 254 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7323714\n",
      "Epoch 255 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7305982\n",
      "Epoch 256 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.72884107\n",
      "Epoch 257 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7270805\n",
      "Epoch 258 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.72532296\n",
      "Epoch 259 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.72356796\n",
      "Epoch 260 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7218077\n",
      "Epoch 261 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7200493\n",
      "Epoch 262 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.71830994\n",
      "Epoch 263 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.71657735\n",
      "Epoch 264 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7148378\n",
      "Epoch 265 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.71310514\n",
      "Epoch 266 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7113802\n",
      "Epoch 267 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7096618\n",
      "Epoch 268 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7079355\n",
      "Epoch 269 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7062095\n",
      "Epoch 270 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.70449525\n",
      "Epoch 271 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.70279545\n",
      "Epoch 272 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.7010879\n",
      "Epoch 273 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.69938594\n",
      "Epoch 274 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.697692\n",
      "Epoch 275 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.69599944\n",
      "Epoch 276 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6943204\n",
      "Epoch 277 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6926427\n",
      "Epoch 278 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6909626\n",
      "Epoch 279 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6892891\n",
      "Epoch 280 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6876119\n",
      "Epoch 281 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6859425\n",
      "Epoch 282 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6842694\n",
      "Epoch 283 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.682606\n",
      "Epoch 284 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6809621\n",
      "Epoch 285 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.67930764\n",
      "Epoch 286 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6776541\n",
      "Epoch 287 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.67600083\n",
      "Epoch 288 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6743558\n",
      "Epoch 289 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.67272824\n",
      "Epoch 290 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.67109466\n",
      "Epoch 291 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.66946644\n",
      "Epoch 292 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.66785264\n",
      "Epoch 293 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6662307\n",
      "Epoch 294 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.66461015\n",
      "Epoch 295 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6629905\n",
      "Epoch 296 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6613768\n",
      "Epoch 297 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6597801\n",
      "Epoch 298 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6581878\n",
      "Epoch 299 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6565922\n",
      "Epoch 300 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6549906\n",
      "Epoch 301 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6533977\n",
      "Epoch 302 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6518093\n",
      "Epoch 303 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6502216\n",
      "Epoch 304 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.64864856\n",
      "Epoch 305 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6470749\n",
      "Epoch 306 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.64550406\n",
      "Epoch 307 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.64394003\n",
      "Epoch 308 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.642381\n",
      "Epoch 309 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.64082533\n",
      "Epoch 310 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6392706\n",
      "Epoch 311 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.63771856\n",
      "Epoch 312 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.636161\n",
      "Epoch 313 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6346113\n",
      "Epoch 314 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6330798\n",
      "Epoch 315 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6315442\n",
      "Epoch 316 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6300133\n",
      "Epoch 317 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6284853\n",
      "Epoch 318 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.62696177\n",
      "Epoch 319 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6254404\n",
      "Epoch 320 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6239267\n",
      "Epoch 321 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6224095\n",
      "Epoch 322 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.62090486\n",
      "Epoch 323 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6194083\n",
      "Epoch 324 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6179045\n",
      "Epoch 325 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.61640036\n",
      "Epoch 326 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6149028\n",
      "Epoch 327 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6134166\n",
      "Epoch 328 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.61193854\n",
      "Epoch 329 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6104543\n",
      "Epoch 330 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6089664\n",
      "Epoch 331 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6074847\n",
      "Epoch 332 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6060122\n",
      "Epoch 333 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6045448\n",
      "Epoch 334 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.6030846\n",
      "Epoch 335 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.60163385\n",
      "Epoch 336 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.60018337\n",
      "Epoch 337 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.59873146\n",
      "Epoch 338 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5972826\n",
      "Epoch 339 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5958299\n",
      "Epoch 340 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5943835\n",
      "Epoch 341 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.59295136\n",
      "Epoch 342 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5915165\n",
      "Epoch 343 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.59008294\n",
      "Epoch 344 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.58864677\n",
      "Epoch 345 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5872241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.58580166\n",
      "Epoch 347 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.58437943\n",
      "Epoch 348 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5829689\n",
      "Epoch 349 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5815661\n",
      "Epoch 350 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5801688\n",
      "Epoch 351 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.57876664\n",
      "Epoch 352 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.57736707\n",
      "Epoch 353 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5759759\n",
      "Epoch 354 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5745895\n",
      "Epoch 355 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.573204\n",
      "Epoch 356 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.571816\n",
      "Epoch 357 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5704352\n",
      "Epoch 358 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5690598\n",
      "Epoch 359 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.56769055\n",
      "Epoch 360 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5663275\n",
      "Epoch 361 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5649628\n",
      "Epoch 362 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5635921\n",
      "Epoch 363 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.56223345\n",
      "Epoch 364 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5608756\n",
      "Epoch 365 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.55952156\n",
      "Epoch 366 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.55818135\n",
      "Epoch 367 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.55683666\n",
      "Epoch 368 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.55549264\n",
      "Epoch 369 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5541522\n",
      "Epoch 370 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.55281776\n",
      "Epoch 371 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5514911\n",
      "Epoch 372 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5501735\n",
      "Epoch 373 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5488543\n",
      "Epoch 374 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5475413\n",
      "Epoch 375 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.54622954\n",
      "Epoch 376 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5449155\n",
      "Epoch 377 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5436082\n",
      "Epoch 378 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5423074\n",
      "Epoch 379 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5410028\n",
      "Epoch 380 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.53970474\n",
      "Epoch 381 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.53841585\n",
      "Epoch 382 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.537123\n",
      "Epoch 383 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5358358\n",
      "Epoch 384 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5345523\n",
      "Epoch 385 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.53326863\n",
      "Epoch 386 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.53199124\n",
      "Epoch 387 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5307167\n",
      "Epoch 388 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5294442\n",
      "Epoch 389 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.52818036\n",
      "Epoch 390 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5269175\n",
      "Epoch 391 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5256612\n",
      "Epoch 392 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5244079\n",
      "Epoch 393 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.52315193\n",
      "Epoch 394 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.52189964\n",
      "Epoch 395 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5206547\n",
      "Epoch 396 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.51940864\n",
      "Epoch 397 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5181701\n",
      "Epoch 398 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5169389\n",
      "Epoch 399 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5157039\n",
      "Epoch 400 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5144734\n",
      "Epoch 401 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5132502\n",
      "Epoch 402 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5120271\n",
      "Epoch 403 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5108107\n",
      "Epoch 404 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.50959796\n",
      "Epoch 405 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5083788\n",
      "Epoch 406 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5071731\n",
      "Epoch 407 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5059679\n",
      "Epoch 408 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5047593\n",
      "Epoch 409 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5035574\n",
      "Epoch 410 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.50236696\n",
      "Epoch 411 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.5011738\n",
      "Epoch 412 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.49998483\n",
      "Epoch 413 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.498799\n",
      "Epoch 414 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.49761534\n",
      "Epoch 415 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4964371\n",
      "Epoch 416 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.49526605\n",
      "Epoch 417 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.49409166\n",
      "Epoch 418 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4929273\n",
      "Epoch 419 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.49176565\n",
      "Epoch 420 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.490599\n",
      "Epoch 421 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.48944053\n",
      "Epoch 422 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4882892\n",
      "Epoch 423 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.48713398\n",
      "Epoch 424 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4859827\n",
      "Epoch 425 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4848275\n",
      "Epoch 426 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.48367894\n",
      "Epoch 427 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.48254344\n",
      "Epoch 428 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.48140907\n",
      "Epoch 429 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.48027384\n",
      "Epoch 430 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47914335\n",
      "Epoch 431 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4780158\n",
      "Epoch 432 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47689328\n",
      "Epoch 433 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47577426\n",
      "Epoch 434 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47465417\n",
      "Epoch 435 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47353163\n",
      "Epoch 436 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47241494\n",
      "Epoch 437 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47130248\n",
      "Epoch 438 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.47019902\n",
      "Epoch 439 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46910012\n",
      "Epoch 440 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46800652\n",
      "Epoch 441 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4669118\n",
      "Epoch 442 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46581817\n",
      "Epoch 443 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46472505\n",
      "Epoch 444 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46363744\n",
      "Epoch 445 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46255597\n",
      "Epoch 446 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.46147546\n",
      "Epoch 447 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.460402\n",
      "Epoch 448 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4593329\n",
      "Epoch 449 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4582571\n",
      "Epoch 450 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.45719096\n",
      "Epoch 451 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4561245\n",
      "Epoch 452 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.45505777\n",
      "Epoch 453 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4539937\n",
      "Epoch 454 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.45293954\n",
      "Epoch 455 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.45188114\n",
      "Epoch 456 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.45082727\n",
      "Epoch 457 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.44977626\n",
      "Epoch 458 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.44873548\n",
      "Epoch 459 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.44769847\n",
      "Epoch 460 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4466618\n",
      "Epoch 461 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4456259\n",
      "Epoch 462 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4445939\n",
      "Epoch 463 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.44356933\n",
      "Epoch 464 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4425445\n",
      "Epoch 465 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.44151816\n",
      "Epoch 466 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.44049895\n",
      "Epoch 467 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4394791\n",
      "Epoch 468 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43846533\n",
      "Epoch 469 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4374546\n",
      "Epoch 470 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43644932\n",
      "Epoch 471 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43544403\n",
      "Epoch 472 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43444386\n",
      "Epoch 473 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43344507\n",
      "Epoch 474 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43244585\n",
      "Epoch 475 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43145037\n",
      "Epoch 476 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.43045735\n",
      "Epoch 477 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42946684\n",
      "Epoch 478 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4284769\n",
      "Epoch 479 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42749462\n",
      "Epoch 480 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42651352\n",
      "Epoch 481 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42553803\n",
      "Epoch 482 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42456594\n",
      "Epoch 483 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42359725\n",
      "Epoch 484 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42263293\n",
      "Epoch 485 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42167065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.42070517\n",
      "Epoch 487 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41974625\n",
      "Epoch 488 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4187881\n",
      "Epoch 489 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41783676\n",
      "Epoch 490 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41688895\n",
      "Epoch 491 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41593626\n",
      "Epoch 492 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41498518\n",
      "Epoch 493 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41404143\n",
      "Epoch 494 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41309953\n",
      "Epoch 495 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4121599\n",
      "Epoch 496 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.41122702\n",
      "Epoch 497 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4102964\n",
      "Epoch 498 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.40936768\n",
      "Epoch 499 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.40844464\n",
      "Epoch 500 : Acc Train Pos:  1.0 Acc Train Neg:  0.0 Loss:  0.4075251\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Enter your answers for 1.1 and 1.2 here: https://tinyurl.com/y5eo334s\n",
    "\n",
    "### 1.1 BPR Loss\n",
    "\n",
    "In the mf_reader() function, implement the BPR loss as an alternative to the logistic loss.\n",
    "\n",
    "### 1.2 Alternative Negative Sampling\n",
    "\n",
    "Re-visit the negative sampling strategy implemented in the function read_filtered_records(). Think about what a good alternative or refinement to it would be and implement it.\n",
    "\n",
    "### 2.1 Testing on Test Data\n",
    "\n",
    "The above code tests on the training data. This is good for sanity checking purposes, but doesn't give real test results. Split the dataset into two parts (80%, 20%) and evaluate on the test data.\n",
    "\n",
    "### 2.2 Hyperparameter Tuning\n",
    "\n",
    "Now split the data into three parts (80%, 10%, 10%), i.e. a training, development and test set. You can use the development set to test hyperparameters of your model -- embedding dimensionality, number of epochs, etc. You can also keep the number of epochs flexible and perform early stopping, i.e. stop training once the model has reached a certain accuracy or loss.\n",
    "\n",
    "### 2.3 Training on the Full Dataset\n",
    "\n",
    "Revisit read_filtered_records() and change the function such that it reads in the whole dataset, or at least a larger portion of it. Re-train your models and record performances. Reflect on what worked well and what didn't, and think about why that might be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
